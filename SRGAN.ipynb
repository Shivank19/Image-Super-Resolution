{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, PReLU, BatchNormalization, Flatten, UpSampling2D, LeakyReLU, Dense, Input, add\n",
    "from keras.applications import VGG19\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Block for Generator\n",
    "def residual(ip):\n",
    "    res_model = Conv2D(64, (3, 3), padding = \"same\")(ip)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "    res_model = PReLU(shared_axes = [1, 2])(res_model)\n",
    "    \n",
    "    res_model = Conv2D(64, (3, 3), padding = \"same\")(res_model)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "    return add([ip, res_model])\n",
    "\n",
    "#Upscaling Block for generator\n",
    "def upscale(ip):\n",
    "    up_model = Conv2D(256, (3, 3), padding = \"same\")(ip)\n",
    "    up_model = UpSampling2D(size = 2)(up_model)\n",
    "    up_model = PReLU(shared_axes = [1, 2])(up_model)\n",
    "    \n",
    "    return up_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR MODEL\n",
    "def generator_model(generator_ip, num_res_block):\n",
    "    layers = Conv2D(64, (9, 9), padding = \"same\")(generator_ip)\n",
    "    layers = PReLU(shared_axes = [1, 2])(layers)\n",
    "\n",
    "    temp = layers\n",
    "\n",
    "    for i in range(num_res_block):\n",
    "        layers = residual(layers)\n",
    "    \n",
    "    layers = Conv2D(64, (3, 3), padding = \"same\")(layers)\n",
    "    layers = BatchNormalization(momentum = 0.5)(layers)\n",
    "    layers = add([layers, temp])\n",
    "\n",
    "    layers = upscale(layers)\n",
    "    layers = upscale(layers)\n",
    "\n",
    "    op = Conv2D(3, (9, 9), padding = \"same\")(layers)\n",
    "    return Model(inputs = generator_ip, outputs = op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_block(ip, filters, strides=1, bn=True):\n",
    "    disc_model = Conv2D(filters, (3, 3), strides = strides, padding = \"same\")(ip)\n",
    "    if bn: \n",
    "        disc_model = BatchNormalization(momentum = 0.8)(disc_model)\n",
    "    \n",
    "    disc_model = LeakyReLU(alpha = 0.2)(disc_model)\n",
    "    return disc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(disc_ip):\n",
    "    df = 64\n",
    "    d1 = disc_block(disc_ip, df, bn=False)\n",
    "    d2 = disc_block(d1, df, strides=2)\n",
    "    d3 = disc_block(d2, df*2)\n",
    "    d4 = disc_block(d3, df*2, strides=2)\n",
    "    d5 = disc_block(d4, df*4)\n",
    "    d6 = disc_block(d5, df*4, strides=2)\n",
    "    d7 = disc_block(d6, df*8)\n",
    "    d8 = disc_block(d7, df*8, strides=2)\n",
    "    d8_5 = Flatten()(d8)\n",
    "    d9 = Dense(df*16)(d8_5)\n",
    "    d10 = LeakyReLU(alpha = 0.2)(d9)\n",
    "\n",
    "    validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "    return Model(disc_ip, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg(hr_shape):\n",
    "    vgg = VGG19(weights=\"imagenet\", include_top=False,input_shape=hr_shape)\n",
    "    return Model(inputs=vgg.inputs, outputs=vgg.layers[10].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_model(gen, disc, vgg, lr_ip, hr_ip):\n",
    "    gen_img = gen(lr_ip)\n",
    "    gen_features = vgg(gen_img)\n",
    "    disc.trainable = False\n",
    "    validity = disc(gen_img)\n",
    "    return Model(inputs=[lr_ip, hr_ip], outputs=[validity, gen_features]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "#Loading Low Resolution (Downscaled) images for training\n",
    "lr_list = os.listdir(\"data/lr_images/\")[:n]\n",
    "lr_images = []\n",
    "for img in lr_list:\n",
    "    img_lr = cv2.imread(\"data/lr_images/\" + img)\n",
    "    img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2RGB)\n",
    "    lr_images.append(img_lr)\n",
    "\n",
    "#Loading High Resolution (Downscaled) images for training\n",
    "hr_list = os.listdir(\"data/hr_images/\")[:n]\n",
    "hr_images = []\n",
    "for img in hr_list: \n",
    "    img_hr = cv2.imread(\"data/hr_images/\" + img)\n",
    "    img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2RGB)\n",
    "    hr_images.append(img_hr)\n",
    "\n",
    "lr_images = np.array(lr_images)\n",
    "hr_images = np.array(hr_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check on Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_no = random.randint(0, len(lr_images) - 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(lr_images[img_no], (64, 64, 3)))\n",
    "plt.title('Low Resolution Image (64x64)')\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(hr_images[img_no], (256, 256, 3)))\n",
    "plt.title('High Resolution Image (256x256)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Values\n",
    "lr_images = lr_images / 255 # type: ignore\n",
    "hr_images = hr_images / 255 # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train, lr_test, hr_train, hr_test = train_test_split(lr_images, hr_images, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting shape for LR and HR images to pass to generator model\n",
    "hr_shape = (hr_train.shape[1], hr_train.shape[2], hr_train.shape[3]) # type: ignore\n",
    "lr_shape = (lr_train.shape[1], lr_train.shape[2], lr_train.shape[3]) # type: ignore\n",
    "\n",
    "lr_ip = Input(shape = lr_shape)\n",
    "hr_ip = Input(shape = hr_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model(lr_ip, num_res_block=16)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_model(hr_ip)\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = build_vgg((256, 256, 3))\n",
    "print(vgg.summary())\n",
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model = comb_model(generator, discriminator, vgg, lr_ip, hr_ip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 losses : Adversarial loss and Content (VGG) loss\n",
    "* **Adversarial Loss**: is defined based on the probabilities of the discriminator over all training samples use binary_crossentropy\n",
    "\n",
    "* **Content Loss**: feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network MSE between the feature representations of a reconstructed image and the reference image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model.compile(loss=[\"binary_crossentropy\", \"mse\"], loss_weights=[1e-3, 1], optimizer=\"adam\")\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating batches of images to be fetched during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_lr_batches = []\n",
    "train_hr_batches = []\n",
    "for i in range(int(hr_train.shape[0] / batch_size)):    # type: ignore\n",
    "    start_index = i * batch_size\n",
    "    end_index = start_index + batch_size\n",
    "    train_hr_batches.append(hr_train[start_index:end_index])\n",
    "    train_lr_batches.append(lr_train[start_index:end_index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "#Train over epochs:\n",
    "for e in range(epochs): \n",
    "    #Assign label 0 to generated (fake) images\n",
    "    fake_label = np.zeros((batch_size, 1))\n",
    "    #Assign label 1 to real images\n",
    "    real_label = np.ones((batch_size, 1))\n",
    "\n",
    "    #Lists to populate with generator and discriminator losses\n",
    "    g_losses, d_losses = [], []\n",
    "\n",
    "    #enumerate training over batches\n",
    "    for b in tqdm(range(len(train_hr_batches))):\n",
    "        #Fetch a batch of Low Resolution images for training\n",
    "        lr_imgs = train_lr_batches[b]\n",
    "        #Fetch a batch of High Resolution images for training\n",
    "        hr_imgs = train_hr_batches[b]\n",
    "\n",
    "        fake_imgs = generator.predict_on_batch(lr_imgs)\n",
    "\n",
    "        #train the discriminator on fake and real HR images to classify between real and fake HR images. \n",
    "        discriminator.trainable = True\n",
    "        d_loss_gen = discriminator.train_on_batch(fake_imgs, fake_label)\n",
    "        d_loss_real = discriminator.train_on_batch(hr_imgs, real_label)\n",
    "\n",
    "        #Set discriminatornon-trainable to train the generator\n",
    "        discriminator.trainable = False\n",
    "        #Average discriminator loss\n",
    "        d_loss = 0.5 * np.add(d_loss_gen, d_loss_real)\n",
    "\n",
    "        #Extract VGG Features to calculate loss\n",
    "        img_features = vgg.predict(hr_imgs)\n",
    "\n",
    "        #Train the generator\n",
    "        g_loss, _, _ = gan_model.train_on_batch([lr_imgs, hr_imgs], [real_label, img_features])\n",
    "\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "    \n",
    "    g_losses = np.array(g_losses)\n",
    "    d_losses = np.array(d_losses)\n",
    "\n",
    "    #Average Losses for Generator and Discriminator\n",
    "    g_loss = np.sum(g_losses, axis=0) / len(g_losses)\n",
    "    d_loss = np.sum(d_losses, axis=0) / len(d_losses)\n",
    "\n",
    "    print(f\"Epoch: {e + 1}, Generator Loss: {g_loss}, Discriminator Loss: {d_loss}\")\n",
    "\n",
    "    # if (e + 1) % 5 == 0: \n",
    "    generator.save(\"gen_e_\" + \"_\" +str(e + 1) + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isr",
   "language": "python",
   "name": "isr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc3f5e30da040d65511822191e49a07fc516e5fea13bc89cd3f674d171a509d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
